# Crawling_크롤링

### 1. 개요

- 크롤링 : 데이터를 불러오는 것

- 파싱 : 크롤링한 데이터에서 원하는 정보를 뽑아내는 것 (웹의 응답)



### 2. 웹에서 데이터 수집하는 방법

- 목적 : **분석**에 필요한 **데이터** 수집
  
  1. 웹사이트 직접 접근
  - 원하는 만큼 데이터 수집할 수 있으나, 직접 가공해야 함.     
  
  - 한계 : 법적 문제 발생 / UI 변경 시, 모두 수정해야 함
  2. <mark>API 호출</mark> 👈 **추천**!
  - 한계 : 제한된 용량 및 유료화
  
  - 상태코드를 유의해야 함 : 200 이 크롤링 가능

- 취준생을 위한 데이터 수집 : API 방식 추천



### 3. 크롤링 할 때 주의사항

- 웹사이트 직접 접근 시, `robots.txt` 확인 필수
  
  - 각 사이트마다 허용범위가 다 다름
  
  - 계속 수집 시, IP 차단 가능성 존재

- 따라서, API 로 학습하는 것이 안전함



### 4. 웹 크롤링 프로세스

| 절차            | 내용                                                                                 | 주요 파이썬 라이브러리                          |
| ------------- | ---------------------------------------------------------------------------------- | ------------------------------------- |
| HTTP Request  | GET, POST 방식의 HTTP 통신                                                              | - request<br/>- selenium<br/>- scrapy |
| HTTP Response | - 응답 결과 확인(상태코드, 인코딩 방식)<br/>- 응답 받은 객체를 텍스트로 출력 <br/>- 응답 받은 객체에 찾는 HTML 포함 여부 확인 |                                       |
| HTML에서 데이터 추출 | - 응답받은 객체를 HTML 로 변환<br/>- CSS 또는 XPath로 HTML 요소 찾기<br/>- HTML 요소로부터 데이터 추출        | - scrapy<br/>- beautifulsoup          |
| 데이터 전처리 및 저장  | - 텍스트 전처리, 분리<br/>- 데이터 프레임으로 변환<br/>- DB에 저장                                      | pandas                                |

 

#### * 각 처리 공정 단계

1. 수집 : 네트워크에 있는 데이터 요청.

2. 분석 : 웹사이트 구조 분석 및 파싱, 텍스트 데이터를 구조화된 데이터로 변환

3. 추출 : 필요한 데이터를 특정 키를 기반으로 추출하거나, 스크래이핑 또는 정규 표현식 사용

4. 가공 : 사용 형태에 맞게 추출한 데이터에서 노이즈 제거, 정규화, 이미지 처리 등 진행

5. 저장 : 수집한 페이지와 추출하고, 가공한 데이터를 데이터 저장소에 저장

         ▶︎ pandas **dataframe**으로 변환 후 SQL 사용



#### * 크롤링을 위해 꼭 알아야 할 것

- 크롬 개발자 도구

- 문자열 처리(정규표현식)

- API 문서 확인

- 인코딩 이슈



#### * API 공식문서 확인

- API 결과 응답방식 : XML, JSON 등 명시되어 있음

- 꼼꼼히 읽어보고, 분석에 활용해야 함

---

## 5. 크롤링 주요 라이브러리

- **`Selenium`** : 간편한 도구
  
  - 동적 웹 콘텐츠 스크래핑 하는데 널리 사용
  
  - 버튼 클릭, 양식 작성 등 인간의 상호작용 모방
  
  - chrome, firefox 등 다양한 브라우저와 호환
  
  - 상태코드 등은 확인할 수 없음
  
  - 주요 메서드
    
    - find_element() or find_elements()

- **`Requests`**
  
  - 사용자 친화적인 웹 스크래핑 라이브러리, urllib3 위에 구축
  
  - get 메서드 사용 시, 응답된 웹 페이지에 접근 가능
  
  - 단, javascript와 같은 동적 페이지에는 적용이 안됨
  
  - 특정 정보를 가져올 경우 BeautifulSoup 라이브러리 필요
  
  - 주요 메서드 : get(), post()

- **`BeautifulSoup`**
  
  - XML 및 HTML 문서 파싱을 위한 라이브러리
  
  - 편리함이 가장 큰 장점
  
  - 앞서 소개한 Requests 라이브러리가 같이 사용도미
  
  - 주요 메서드 : find_all(), get_text()

- **`Scrapy`** : 전문적 크롤링 프레임워크 개념 (scrapy framework)
  
  - 크롤링 프레임 워크
  
  - 객체 지향 프로그래밍으로, 클래스를 잘 알아야 함
  
  - Scrapy 명령어를 터미널에 입력해야 함
  
  - CSS Selector와 Xpath를 잘 사용해야 함
  
  - Spider
    
    - scrapy.Spider : 처음부터 하나씩 만들기
    
    - Crawlspider : 기본 기능 제공
    
    - 반환값은 리스트 자료형으로 처리
      
      - `response.xpath().get()` : 하나의 값만 반환
      
      - `response.xpath().getall()` : 여러개의 값 반환

 

---

## 6. Xpath 문법 25페이지

- Xpath = XML path language

- XML 문서에서 정보를 탐색, 추출하는데 사용되는 쿼리 언어

| 주요 문법                           | 내용                                               |
|:------------------------------- |:------------------------------------------------ |
| //tagname                       | 페이지 내 특정 태그 이름 모두 선택                             |
| //tagname[1]                    | 중복되는 특정 태그 중 한 개 선택                              |
| //tagname[@태그 내 속성 = "속성값"]     | 특정 태그의 특정 속성 선택 (예 : //div[@class = "container"] |
| contains("XML", "X") => true    | 첫번째 문자열이 두번째 문자열을 포함하고 있으면 참, 그외는 거짓             |
| starts-with("XML", "X") => true | 첫번째 문자열이 두번째 문자열로 시작하면 참, 그외는 거짓                 |
| //tagname[(표현식1) and/or (표현식2)] | //div[@class="container" and @id="second"]       |



#### * 테스트하기 좋은 사이트

-  https://www.w3schools.com/xml/xpath_syntax.asp  

- Xpath Playgroud : https://scrapinghub.github.io/xpath-playground/ 

- Xpath CheetSheet : https://devhints.io/xpath
